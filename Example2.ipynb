{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - MNIST with a Shallow Feedforward Network\n",
    "We will use a shallow feedforward neural network to learn the famous MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, as usual, we import Numpy to hold the data, and we import Learny McLearnface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import LearnyMcLearnface as lml\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will set up the MNIST dataset.  This dataset consists of a training set of 60,000 28x28 images of handwritten digits, along with a test set of 10,000 different images of the same size.  We will use a built-in utility to import the data.\n",
    "\n",
    "Since these images are 28x28 grayscale pixels, they will be stretched into single dimensional vectors of length 784.\n",
    "Then, the training and test image sets are combined into one large matrix each, where each row is a single 784 image.\n",
    "\n",
    "There are also two vectors of image classifications (lengths 60,000 and 10,000) which correspond to these two matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_images, training_labels, test_images, test_labels = lml.utils.get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is common to have separate validation and test sets, so we will split MNIST's test set in half to accomodate this. The first 5000 images will be our validation set, and the rest will be our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = training_images\n",
    "y_train = training_labels\n",
    "X_val = test_images[:5000, :]\n",
    "y_val = test_labels[:5000]\n",
    "X_test = test_images[5000:, :]\n",
    "y_test = test_labels[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we wrap our training and validation sets in a dictionary as usual, so that we may feed it to Learny McLearnface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'X_train' : X_train,\n",
    "    'y_train' : y_train,\n",
    "    'X_val' : X_val,\n",
    "    'y_val' : y_val\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our model. We will use a similar architecture to Example 1: a shallow fully-connected network with 500 hidden layer neurons, ReLU activations, and a Softmax classifier.\n",
    "\n",
    "Since this network will be taking 28x28 images, its input dimension will be 28*28. We will use the Xavier scheme to initialize our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'input_dim' : 28*28,\n",
    "    'init_scheme' : 'xavier'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = lml.NeuralNetwork(opts)\n",
    "nn.add_layer('Affine', {'neurons':600})\n",
    "nn.add_layer('ReLU', {})\n",
    "nn.add_layer('Affine', {'neurons':10})\n",
    "nn.add_layer('SoftmaxLoss', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we will use a Trainer in order to fit the model to the MNIST dataset. We will use stochastic gradient descent, and we will use a learning rate of 1e-2 and a regularization constant of 1e-8. We will also train the model for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'update_options' : {'update_rule' : 'sgd', 'learning_rate' : 1e-2},\n",
    "    'reg_param' : 1e-8,\n",
    "    'num_epochs' : 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use a Trainer object in order to train the model to the data, with the chosen options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = lml.Trainer(nn, data, opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the model once before training. Since the model is randomly initialized, we will expect the model's predictions to be essentially random as well. As there are 10 classes, we will expect an initial accuracy roughly near 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model accuracy: 0.1242\n"
     ]
    }
   ],
   "source": [
    "accuracy = trainer.accuracy(X_val, y_val)\n",
    "print('Initial model accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Documents\\Programs\\Python\\Deep-Learning\\LearnyMcLearnface\\Layers\\SoftmaxLossLayer.py:26: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = np.sum(-np.log(probabilities[(range(N), y)])) / N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 Validation accuracy: 0.8348\n",
      "Epoch 2 of 10 Validation accuracy: 0.9128\n",
      "Epoch 3 of 10 Validation accuracy: 0.8846\n",
      "Epoch 4 of 10 Validation accuracy: 0.8758\n",
      "Epoch 5 of 10 Validation accuracy: 0.9216\n",
      "Epoch 6 of 10 Validation accuracy: 0.9276\n",
      "Epoch 7 of 10 Validation accuracy: 0.926\n",
      "Epoch 8 of 10 Validation accuracy: 0.9312\n",
      "Epoch 9 of 10 Validation accuracy: 0.9372\n",
      "Epoch 10 of 10 Validation accuracy: 0.9336\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And then, we print the final test set accuracy. With the hyperparameters we used earlier, we can expect a test set accuracy of roughly 92-95%. It is clear that the model has fit the dataset pretty well. However, with a bit of hyperparameter optimization, it can do even better. Feel free to experiment with the hyperparameter values, and try to beat our final accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model accuracy: 0.961\n"
     ]
    }
   ],
   "source": [
    "accuracy = trainer.accuracy(X_test, y_test)\n",
    "print('Initial model accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
