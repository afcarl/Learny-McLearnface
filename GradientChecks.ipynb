{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Gradient Checks\n",
    "\n",
    "Here, we use numerical gradient checking to verify the backpropagation correctness of all layers in the Layers folder.  We should expect to see very small nonzero values for error, as the checking process approximates the gradient numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import LearnyMcLearnface as lml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine Layer\n",
    "Layers/AffineLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affine dx error: 7.17763668045e-08\n",
      "Affine dW error: 2.70895792014e-05\n",
      "Affine db error: 1.26310646624e-08\n"
     ]
    }
   ],
   "source": [
    "affine = lml.layers.AffineLayer(30, 10, 1e-2)\n",
    "test_input = np.random.randn(50, 30)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = affine.forward(test_input)\n",
    "dx_num = lml.utils.numerical_gradient_layer(lambda x : affine.forward(x, affine.W, affine.b), test_input, dout)\n",
    "dW_num = lml.utils.numerical_gradient_layer(lambda w : affine.forward(test_input, w, affine.b), affine.W, dout)\n",
    "db_num = lml.utils.numerical_gradient_layer(lambda b : affine.forward(test_input, affine.W, b), affine.b, dout)\n",
    "dx = affine.backward(dout)\n",
    "print('Affine dx error:', np.max(lml.utils.relative_error(dx, dx_num)))\n",
    "print('Affine dW error:', np.max(lml.utils.relative_error(affine.dW, dW_num)))\n",
    "print('Affine db error:', np.max(lml.utils.relative_error(affine.db, db_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Layer\n",
    "\n",
    "Layers/DropoutLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout dx error: 3.38583654876e-12\n"
     ]
    }
   ],
   "source": [
    "dropout = lml.layers.DropoutLayer(10, 0.6, seed=5684)\n",
    "test_input = np.random.randn(3, 10)\n",
    "dout = np.random.randn(3, 10)\n",
    "_ = dropout.forward_train(test_input)\n",
    "dx_num = lml.utils.numerical_gradient_layer(lambda x : dropout.forward_train(x), test_input, dout)\n",
    "dx = dropout.backward(dout)\n",
    "print('Dropout dx error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit) Layer\n",
    "Layers/ReLULayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU dx error: 3.27562981159e-12\n"
     ]
    }
   ],
   "source": [
    "relu = lml.layers.ReLULayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = relu.forward(test_input)\n",
    "dx_num = lml.utils.numerical_gradient_layer(lambda x : relu.forward(x), test_input, dout)\n",
    "dx = relu.backward(dout)\n",
    "print('ReLU dx error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Layer\n",
    "Layers/SigmoidLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid dx error: 4.097386703e-11\n"
     ]
    }
   ],
   "source": [
    "sigmoid = lml.layers.SigmoidLayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = sigmoid.forward(test_input)\n",
    "dx_num = lml.utils.numerical_gradient_layer(lambda x : sigmoid.forward(x), test_input, dout)\n",
    "dx = sigmoid.backward(dout)\n",
    "print('Sigmoid dx error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Loss Layer\n",
    "Layers/SoftmaxLossLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax backprop error: 9.67306817589e-08\n"
     ]
    }
   ],
   "source": [
    "softmax = lml.layers.SoftmaxLossLayer(10)\n",
    "test_scores = np.random.randn(50, 10)\n",
    "test_classes = np.random.randint(1, 10, 50)\n",
    "_, dx = softmax.loss(test_scores, test_classes)\n",
    "dx_num = lml.utils.numerical_gradient(lambda x : softmax.loss(x, test_classes)[0], test_scores)\n",
    "print('Softmax backprop error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Loss Layer\n",
    "\n",
    "Layers/SVMLossLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM backprop error: 1.40215660067e-09\n"
     ]
    }
   ],
   "source": [
    "svm = lml.layers.SVMLossLayer(10)\n",
    "test_scores = np.random.randn(50, 10)\n",
    "test_classes = np.random.randint(1, 10, 50)\n",
    "_, dx = svm.loss(test_scores, test_classes)\n",
    "dx_num = lml.utils.numerical_gradient(lambda x : svm.loss(x, test_classes)[0], test_scores)\n",
    "print('SVM backprop error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Layer\n",
    "\n",
    "Layers/TanhLayer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "tanh = lml.layers.TanhLayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = tanh.forward(test_input)\n",
    "dx_num = lml.utils.numerical_gradient_layer(lambda x : tanh.forward(x), test_input, dout)\n",
    "dx = tanh.backward(dout)\n",
    "print('Tanh dx error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Model Gradient Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Layer Network\n",
    "This is a gradient check for a simple example network with the following architecture:\n",
    "Affine, ReLU, Affine, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With regularization off:\n",
      "Weight 1 error: 0.000266447122624\n",
      "Bias 1 error: 4.32867650991e-06\n",
      "Weight 2 error: 3.56849989239e-05\n",
      "Bias 2 error: 2.35135119232e-05\n",
      "With regularization at lambda = 1.0:\n",
      "Weight 1 error: 4.27541260165e-05\n",
      "Bias 1 error: 4.32867650991e-06\n",
      "Weight 2 error: 3.42603091643e-06\n",
      "Bias 2 error: 2.35135119232e-05\n"
     ]
    }
   ],
   "source": [
    "opts = {\n",
    "    'input_dim' : 10,\n",
    "    'data_type' : np.float64\n",
    "}\n",
    "\n",
    "nn = lml.NeuralNetwork(opts)\n",
    "nn.add_layer('Affine', {'neurons':10, 'weight_scale':5e-2})\n",
    "nn.add_layer('ReLU', {})\n",
    "nn.add_layer('Affine', {'neurons':10, 'weight_scale':5e-2})\n",
    "nn.add_layer('SoftmaxLoss', {})\n",
    "test_scores = np.random.randn(20, 10)\n",
    "test_classes = np.random.randint(1, 10, 20)\n",
    "loss, dx = nn.backward(test_scores, test_classes)\n",
    "\n",
    "print('With regularization off:')\n",
    "f = lambda _: nn.backward(test_scores, test_classes)[0]\n",
    "d_b1_num = lml.utils.numerical_gradient(f, nn.layers[0].b, accuracy=1e-8)\n",
    "d_W1_num = lml.utils.numerical_gradient(f, nn.layers[0].W, accuracy=1e-8)\n",
    "print('Weight 1 error:', np.max(lml.utils.relative_error(nn.layers[0].dW, d_W1_num)))\n",
    "print('Bias 1 error:', np.max(lml.utils.relative_error(nn.layers[0].db, d_b1_num)))\n",
    "\n",
    "d_b2_num = lml.utils.numerical_gradient(f, nn.layers[2].b, accuracy=1e-8)\n",
    "d_W2_num = lml.utils.numerical_gradient(f, nn.layers[2].W, accuracy=1e-8)\n",
    "print('Weight 2 error:', np.max(lml.utils.relative_error(nn.layers[2].dW, d_W2_num)))\n",
    "print('Bias 2 error:', np.max(lml.utils.relative_error(nn.layers[2].db, d_b2_num)))\n",
    "\n",
    "print('With regularization at lambda = 1.0:')\n",
    "f = lambda _: nn.backward(test_scores, test_classes, reg_param=1.0)[0]\n",
    "d_b1_num = lml.utils.numerical_gradient(f, nn.layers[0].b, accuracy=1e-8)\n",
    "d_W1_num = lml.utils.numerical_gradient(f, nn.layers[0].W, accuracy=1e-8)\n",
    "print('Weight 1 error:', np.max(lml.utils.relative_error(nn.layers[0].dW, d_W1_num)))\n",
    "print('Bias 1 error:', np.max(lml.utils.relative_error(nn.layers[0].db, d_b1_num)))\n",
    "\n",
    "d_b2_num = lml.utils.numerical_gradient(f, nn.layers[2].b, accuracy=1e-8)\n",
    "d_W2_num = lml.utils.numerical_gradient(f, nn.layers[2].W, accuracy=1e-8)\n",
    "print('Weight 2 error:', np.max(lml.utils.relative_error(nn.layers[2].dW, d_W2_num)))\n",
    "print('Bias 2 error:', np.max(lml.utils.relative_error(nn.layers[2].db, d_b2_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multilayer Fully Connected Network with Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight 1 error: 1.18140579523e-05\n",
      "Bias 1 error: 1.0823116919e-05\n",
      "Weight 2 error: 8.68542286154e-05\n",
      "Bias 2 error: 5.75731489058e-06\n",
      "Weight 3 error: 1.51101874291e-06\n",
      "Bias 3 error: 1.79626723799e-06\n"
     ]
    }
   ],
   "source": [
    "opts = {\n",
    "    'input_dim' : 10,\n",
    "    'data_type' : np.float64,\n",
    "    'init_scheme' : 'xavier'\n",
    "}\n",
    "nn = lml.NeuralNetwork(opts)\n",
    "nn.add_layer('Affine', {'neurons':10})\n",
    "nn.add_layer('ReLU', {})\n",
    "nn.add_layer('Dropout', {'dropout_param':0.85, 'seed':5684})\n",
    "nn.add_layer('Affine', {'neurons':10})\n",
    "nn.add_layer('ReLU', {})\n",
    "nn.add_layer('Dropout', {'dropout_param':0.90, 'seed':5684})\n",
    "nn.add_layer('Affine', {'neurons':10})\n",
    "nn.add_layer('ReLU', {})\n",
    "nn.add_layer('Dropout', {'dropout_param':0.95, 'seed':5684})\n",
    "nn.add_layer('SoftmaxLoss', {})\n",
    "test_scores = np.random.randn(20, 10)\n",
    "test_classes = np.random.randint(1, 10, 20)\n",
    "loss, dx = nn.backward(test_scores, test_classes)\n",
    "\n",
    "f = lambda _: nn.backward(test_scores, test_classes, reg_param=0.7)[0]\n",
    "d_b1_num = lml.utils.numerical_gradient(f, nn.layers[0].b, accuracy=1e-8)\n",
    "d_W1_num = lml.utils.numerical_gradient(f, nn.layers[0].W, accuracy=1e-8)\n",
    "print('Weight 1 error:', np.max(lml.utils.relative_error(nn.layers[0].dW, d_W1_num)))\n",
    "print('Bias 1 error:', np.max(lml.utils.relative_error(nn.layers[0].db, d_b1_num)))\n",
    "\n",
    "d_b1_num = lml.utils.numerical_gradient(f, nn.layers[3].b, accuracy=1e-8)\n",
    "d_W1_num = lml.utils.numerical_gradient(f, nn.layers[3].W, accuracy=1e-8)\n",
    "print('Weight 2 error:', np.max(lml.utils.relative_error(nn.layers[3].dW, d_W1_num)))\n",
    "print('Bias 2 error:', np.max(lml.utils.relative_error(nn.layers[3].db, d_b1_num)))\n",
    "\n",
    "d_b1_num = lml.utils.numerical_gradient(f, nn.layers[6].b, accuracy=1e-8)\n",
    "d_W1_num = lml.utils.numerical_gradient(f, nn.layers[6].W, accuracy=1e-8)\n",
    "print('Weight 3 error:', np.max(lml.utils.relative_error(nn.layers[6].dW, d_W1_num)))\n",
    "print('Bias 3 error:', np.max(lml.utils.relative_error(nn.layers[6].db, d_b1_num)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
