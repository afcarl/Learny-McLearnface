{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Gradient Checks\n",
    "\n",
    "Here, we use numerical gradient checking to verify the backpropagation correctness of all layers in the Layers folder.  We should expect to see very small nonzero values for error, as the checking process approximates the gradient numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from NeuralNetwork import *\n",
    "from Utils.NumericalGradient import *\n",
    "\n",
    "from Layers.SoftmaxLossLayer import *\n",
    "from Layers.AffineLayer import *\n",
    "from Layers.ReLULayer import *\n",
    "from Layers.SigmoidLayer import *\n",
    "from Layers.TanhLayer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine Layer\n",
    "Layers/AffineLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affine dx error: 5.85551142831e-08\n",
      "Affine dW error: 2.70895779983e-05\n",
      "Affine db error: 1.263106277e-08\n"
     ]
    }
   ],
   "source": [
    "affine = AffineLayer(30, 10, 1e-2)\n",
    "test_input = np.random.randn(50, 30)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = affine.forward(test_input)\n",
    "dx_num = numerical_gradient_layer(lambda x : affine.forward(x, affine.W, affine.b), test_input, dout)\n",
    "dW_num = numerical_gradient_layer(lambda w : affine.forward(test_input, w, affine.b), affine.W, dout)\n",
    "db_num = numerical_gradient_layer(lambda b : affine.forward(test_input, affine.W, b), affine.b, dout)\n",
    "dx = affine.backward(dout)\n",
    "print('Affine dx error:', np.max(relative_error(dx, dx_num)))\n",
    "print('Affine dW error:', np.max(relative_error(affine.dW, dW_num)))\n",
    "print('Affine db error:', np.max(relative_error(affine.db, db_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit) Layer\n",
    "Layers/ReLULayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU dx error: 3.27563178309e-12\n"
     ]
    }
   ],
   "source": [
    "relu = ReLULayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = relu.forward(test_input)\n",
    "dx_num = numerical_gradient_layer(lambda x : relu.forward(x), test_input, dout)\n",
    "dx = relu.backward(dout)\n",
    "print('ReLU dx error:', np.max(relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Layer\n",
    "Layers/SigmoidLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid dx error: 5.60067957701e-11\n"
     ]
    }
   ],
   "source": [
    "sigmoid = SigmoidLayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = sigmoid.forward(test_input)\n",
    "dx_num = numerical_gradient_layer(lambda x : sigmoid.forward(x), test_input, dout)\n",
    "dx = sigmoid.backward(dout)\n",
    "print('Sigmoid dx error:', np.max(relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Loss Layer\n",
    "Layers/SoftmaxLossLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax backprop error: 7.30132573919e-08\n"
     ]
    }
   ],
   "source": [
    "softmax = SoftmaxLossLayer(10)\n",
    "test_scores = np.random.randn(50, 10)\n",
    "test_classes = np.random.randint(1, 10, 50)\n",
    "_, dx = softmax.loss(test_scores, test_classes)\n",
    "dx_num = numerical_gradient(lambda x : softmax.loss(x, test_classes)[0], test_scores)\n",
    "print('Softmax backprop error:', np.max(relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Layer\n",
    "\n",
    "Layers/TanhLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanh dx error: 6.88055589118e-06\n"
     ]
    }
   ],
   "source": [
    "tanh = TanhLayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = tanh.forward(test_input)\n",
    "dx_num = numerical_gradient_layer(lambda x : tanh.forward(x), test_input, dout)\n",
    "dx = tanh.backward(dout)\n",
    "print('Tanh dx error:', np.max(relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Layer Network\n",
    "This is a gradient check for a simple example network with the following architecture:\n",
    "Affine, ReLU, Affine, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With regularization off:\n",
      "Weight 1 error: 3.81541867848e-05\n",
      "Bias 1 error: 9.09793829959e-05\n",
      "Weight 2 error: 0.00193133226872\n",
      "Bias 2 error: 4.09544225405e-06\n",
      "With regularization at lambda = 1.0:\n",
      "Weight 1 error: 4.89860336358e-05\n",
      "Bias 1 error: 9.09793829959e-05\n",
      "Weight 2 error: 2.99393089663e-05\n",
      "Bias 2 error: 4.09544225405e-06\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(10, data_type=np.float64)\n",
    "nn.add_layer('Affine', {'neurons':10, 'weight_scale':5e-2})\n",
    "nn.add_layer('ReLU', {})\n",
    "nn.add_layer('Affine', {'neurons':10, 'weight_scale':5e-2})\n",
    "nn.add_layer('SoftmaxLoss', {})\n",
    "test_scores = np.random.randn(20, 10)\n",
    "test_classes = np.random.randint(1, 10, 20)\n",
    "loss, dx = nn.backward(test_scores, test_classes)\n",
    "\n",
    "print('With regularization off:')\n",
    "f = lambda _: nn.backward(test_scores, test_classes)[0]\n",
    "d_b1_num = numerical_gradient(f, nn.layers[0].b, accuracy=1e-8)\n",
    "d_W1_num = numerical_gradient(f, nn.layers[0].W, accuracy=1e-8)\n",
    "print('Weight 1 error:', np.max(relative_error(nn.layers[0].dW, d_W1_num)))\n",
    "print('Bias 1 error:', np.max(relative_error(nn.layers[0].db, d_b1_num)))\n",
    "\n",
    "d_b2_num = numerical_gradient(f, nn.layers[2].b, accuracy=1e-8)\n",
    "d_W2_num = numerical_gradient(f, nn.layers[2].W, accuracy=1e-8)\n",
    "print('Weight 2 error:', np.max(relative_error(nn.layers[2].dW, d_W2_num)))\n",
    "print('Bias 2 error:', np.max(relative_error(nn.layers[2].db, d_b2_num)))\n",
    "\n",
    "print('With regularization at lambda = 1.0:')\n",
    "f = lambda _: nn.backward(test_scores, test_classes, reg_param=1.0)[0]\n",
    "d_b1_num = numerical_gradient(f, nn.layers[0].b, accuracy=1e-8)\n",
    "d_W1_num = numerical_gradient(f, nn.layers[0].W, accuracy=1e-8)\n",
    "print('Weight 1 error:', np.max(relative_error(nn.layers[0].dW, d_W1_num)))\n",
    "print('Bias 1 error:', np.max(relative_error(nn.layers[0].db, d_b1_num)))\n",
    "\n",
    "d_b2_num = numerical_gradient(f, nn.layers[2].b, accuracy=1e-8)\n",
    "d_W2_num = numerical_gradient(f, nn.layers[2].W, accuracy=1e-8)\n",
    "print('Weight 2 error:', np.max(relative_error(nn.layers[2].dW, d_W2_num)))\n",
    "print('Bias 2 error:', np.max(relative_error(nn.layers[2].db, d_b2_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
