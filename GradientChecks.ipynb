{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Gradient Checks\n",
    "\n",
    "Here, we use numerical gradient checking to verify the backpropagation correctness of all layers in the Layers folder.  We should expect to see very small nonzero values for error, as the checking process approximates the gradient numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import LearnyMcLearnface as lml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine Layer\n",
    "Layers/AffineLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affine dx error: 1.77646317238e-08\n",
      "Affine dW error: 2.70895794026e-05\n",
      "Affine db error: 1.26310630116e-08\n"
     ]
    }
   ],
   "source": [
    "affine = lml.layers.AffineLayer(30, 10, 1e-2)\n",
    "test_input = np.random.randn(50, 30)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = affine.forward(test_input)\n",
    "dx_num = lml.utils.numerical_gradient_layer(lambda x : affine.forward(x, affine.W, affine.b), test_input, dout)\n",
    "dW_num = lml.utils.numerical_gradient_layer(lambda w : affine.forward(test_input, w, affine.b), affine.W, dout)\n",
    "db_num = lml.utils.numerical_gradient_layer(lambda b : affine.forward(test_input, affine.W, b), affine.b, dout)\n",
    "dx = affine.backward(dout)\n",
    "print('Affine dx error:', np.max(lml.utils.relative_error(dx, dx_num)))\n",
    "print('Affine dW error:', np.max(lml.utils.relative_error(affine.dW, dW_num)))\n",
    "print('Affine db error:', np.max(lml.utils.relative_error(affine.db, db_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit) Layer\n",
    "Layers/ReLULayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU dx error: 3.27563099096e-12\n"
     ]
    }
   ],
   "source": [
    "relu = lml.layers.ReLULayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = relu.forward(test_input)\n",
    "dx_num = lml.utils.numerical_gradient_layer(lambda x : relu.forward(x), test_input, dout)\n",
    "dx = relu.backward(dout)\n",
    "print('ReLU dx error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Layer\n",
    "Layers/SigmoidLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid dx error: 5.22573395974e-11\n"
     ]
    }
   ],
   "source": [
    "sigmoid = lml.layers.SigmoidLayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = sigmoid.forward(test_input)\n",
    "dx_num = lml.utils.numerical_gradient_layer(lambda x : sigmoid.forward(x), test_input, dout)\n",
    "dx = sigmoid.backward(dout)\n",
    "print('Sigmoid dx error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Loss Layer\n",
    "Layers/SoftmaxLossLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax backprop error: 1.99924891645e-07\n"
     ]
    }
   ],
   "source": [
    "softmax = lml.layers.SoftmaxLossLayer(10)\n",
    "test_scores = np.random.randn(50, 10)\n",
    "test_classes = np.random.randint(1, 10, 50)\n",
    "_, dx = softmax.loss(test_scores, test_classes)\n",
    "dx_num = lml.utils.numerical_gradient(lambda x : softmax.loss(x, test_classes)[0], test_scores)\n",
    "print('Softmax backprop error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Loss Layer\n",
    "\n",
    "Layers/SVMLossLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM backprop error: 1.40215660067e-09\n"
     ]
    }
   ],
   "source": [
    "svm = lml.layers.SVMLossLayer(10)\n",
    "test_scores = np.random.randn(50, 10)\n",
    "test_classes = np.random.randint(1, 10, 50)\n",
    "_, dx = svm.loss(test_scores, test_classes)\n",
    "dx_num = lml.utils.numerical_gradient(lambda x : svm.loss(x, test_classes)[0], test_scores)\n",
    "print('SVM backprop error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Layer\n",
    "\n",
    "Layers/TanhLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanh dx error: 2.13532584756e-06\n"
     ]
    }
   ],
   "source": [
    "tanh = lml.layers.TanhLayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = tanh.forward(test_input)\n",
    "dx_num = lml.utils.numerical_gradient_layer(lambda x : tanh.forward(x), test_input, dout)\n",
    "dx = tanh.backward(dout)\n",
    "print('Tanh dx error:', np.max(lml.utils.relative_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Layer Network\n",
    "This is a gradient check for a simple example network with the following architecture:\n",
    "Affine, ReLU, Affine, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With regularization off:\n",
      "Weight 1 error: 0.000213027114506\n",
      "Bias 1 error: 1.32937266113e-05\n",
      "Weight 2 error: 3.029625769e-05\n",
      "Bias 2 error: 4.14267373061e-05\n",
      "With regularization at lambda = 1.0:\n",
      "Weight 1 error: 1.67206919837e-05\n",
      "Bias 1 error: 1.32937266113e-05\n",
      "Weight 2 error: 4.20328405614e-05\n",
      "Bias 2 error: 4.14267373061e-05\n"
     ]
    }
   ],
   "source": [
    "opts = {\n",
    "    'input_dim' : 10,\n",
    "    'data_type' : np.float64\n",
    "}\n",
    "\n",
    "nn = lml.NeuralNetwork(opts)\n",
    "nn.add_layer('Affine', {'neurons':10, 'weight_scale':5e-2})\n",
    "nn.add_layer('ReLU', {})\n",
    "nn.add_layer('Affine', {'neurons':10, 'weight_scale':5e-2})\n",
    "nn.add_layer('SoftmaxLoss', {})\n",
    "test_scores = np.random.randn(20, 10)\n",
    "test_classes = np.random.randint(1, 10, 20)\n",
    "loss, dx = nn.backward(test_scores, test_classes)\n",
    "\n",
    "print('With regularization off:')\n",
    "f = lambda _: nn.backward(test_scores, test_classes)[0]\n",
    "d_b1_num = lml.utils.numerical_gradient(f, nn.layers[0].b, accuracy=1e-8)\n",
    "d_W1_num = lml.utils.numerical_gradient(f, nn.layers[0].W, accuracy=1e-8)\n",
    "print('Weight 1 error:', np.max(lml.utils.relative_error(nn.layers[0].dW, d_W1_num)))\n",
    "print('Bias 1 error:', np.max(lml.utils.relative_error(nn.layers[0].db, d_b1_num)))\n",
    "\n",
    "d_b2_num = lml.utils.numerical_gradient(f, nn.layers[2].b, accuracy=1e-8)\n",
    "d_W2_num = lml.utils.numerical_gradient(f, nn.layers[2].W, accuracy=1e-8)\n",
    "print('Weight 2 error:', np.max(lml.utils.relative_error(nn.layers[2].dW, d_W2_num)))\n",
    "print('Bias 2 error:', np.max(lml.utils.relative_error(nn.layers[2].db, d_b2_num)))\n",
    "\n",
    "print('With regularization at lambda = 1.0:')\n",
    "f = lambda _: nn.backward(test_scores, test_classes, reg_param=1.0)[0]\n",
    "d_b1_num = lml.utils.numerical_gradient(f, nn.layers[0].b, accuracy=1e-8)\n",
    "d_W1_num = lml.utils.numerical_gradient(f, nn.layers[0].W, accuracy=1e-8)\n",
    "print('Weight 1 error:', np.max(lml.utils.relative_error(nn.layers[0].dW, d_W1_num)))\n",
    "print('Bias 1 error:', np.max(lml.utils.relative_error(nn.layers[0].db, d_b1_num)))\n",
    "\n",
    "d_b2_num = lml.utils.numerical_gradient(f, nn.layers[2].b, accuracy=1e-8)\n",
    "d_W2_num = lml.utils.numerical_gradient(f, nn.layers[2].W, accuracy=1e-8)\n",
    "print('Weight 2 error:', np.max(lml.utils.relative_error(nn.layers[2].dW, d_W2_num)))\n",
    "print('Bias 2 error:', np.max(lml.utils.relative_error(nn.layers[2].db, d_b2_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
