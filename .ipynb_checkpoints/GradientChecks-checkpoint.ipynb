{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Gradient Checks\n",
    "\n",
    "Here, we use numerical gradient checking to verify the backpropagation correctness of all layers in the Layers folder.  We should expect to see very small nonzero values for error, as the checking process approximates the gradient numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from NeuralNetwork import *\n",
    "from Utils.NumericalGradient import *\n",
    "\n",
    "from Layers.SoftmaxLossLayer import *\n",
    "from Layers.AffineLayer import *\n",
    "from Layers.ReLULayer import *\n",
    "from Layers.SigmoidLayer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine Layer\n",
    "Layers/AffineLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affine dx error:  1.51330671561e-08\n",
      "Affine dW error:  1.68484533402e-11\n",
      "Affine db error:  3.61830483695e-12\n"
     ]
    }
   ],
   "source": [
    "affine = AffineLayer(30, 10, 1e-2)\n",
    "test_input = np.random.randn(50, 30)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = affine.forward(test_input)\n",
    "dx_num = numerical_gradient_layer(lambda x : affine.forward(x, affine.W, affine.b), test_input, dout)\n",
    "dW_num = numerical_gradient_layer(lambda w : affine.forward(test_input, w, affine.b), affine.W, dout)\n",
    "db_num = numerical_gradient_layer(lambda b : affine.forward(test_input, affine.W, b), affine.b, dout)\n",
    "dx, dW, db = affine.backward(dout)\n",
    "print 'Affine dx error: ', np.max(relative_error(dx, dx_num))\n",
    "print 'Affine dW error: ', np.max(relative_error(dW, dW_num))\n",
    "print 'Affine db error: ', np.max(relative_error(db, db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit) Layer\n",
    "Layers/ReLULayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU dx error:  3.27563009974e-12\n"
     ]
    }
   ],
   "source": [
    "relu = ReLULayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = relu.forward(test_input)\n",
    "dx_num = numerical_gradient_layer(lambda x : relu.forward(x), test_input, dout)\n",
    "dx = relu.backward(dout)\n",
    "print 'ReLU dx error: ', np.max(relative_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Layer\n",
    "Layers/SigmoidLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid dx error:  5.69203688508e-11\n"
     ]
    }
   ],
   "source": [
    "sigmoid = SigmoidLayer(10)\n",
    "test_input = np.random.randn(50, 10)\n",
    "dout = np.random.randn(50, 10)\n",
    "_ = sigmoid.forward(test_input)\n",
    "dx_num = numerical_gradient_layer(lambda x : sigmoid.forward(x), test_input, dout)\n",
    "dx = sigmoid.backward(dout)\n",
    "print 'Sigmoid dx error: ', np.max(relative_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Loss Layer\n",
    "Layers/SoftmaxLossLayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax backprop error:  1.86304637047e-07\n"
     ]
    }
   ],
   "source": [
    "softmax = SoftmaxLossLayer(10)\n",
    "test_scores = np.random.randn(50, 10)\n",
    "test_classes = np.random.randint(1, 10, 50)\n",
    "_, dx = softmax.loss(test_scores, test_classes)\n",
    "dx_num = numerical_gradient(lambda x : softmax.loss(x, test_classes)[0], test_scores)\n",
    "print 'Softmax backprop error: ', np.max(relative_error(dx, dx_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
